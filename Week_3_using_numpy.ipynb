{"cells":[{"cell_type":"markdown","metadata":{"id":"4lFWhSEEwc9F"},"source":["#MNIST\n","Our objective is to build a neural network for the classification of the MNIST dataset. This neural network will comprise two layers, each with 10 nodes, and an input layer with 784 nodes corresponding to the image pixels. The specific structure of the neural network is outlined below, where $X$ represents the input, $A^{[0]}$ denotes the first layer, $Z^{[1]}$ signifies the unactivated layer 1, $A^{[1]}$ stands for the activated layer 1, and so forth. The weights and biases are represented by $W$ and $b$ respectively:\n"]},{"cell_type":"markdown","metadata":{"id":"dDanK4nEwfhh"},"source":["<div align=\"center\">\n","\n","$A^{[0]}=X$\n","\n","$Z^{[1]}=W^{[1]}A^{[0]}+b^{[1]}$\n","\n","$A^{[1]}=\\text{ReLU}(Z^{[1]})$\n","\n","$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$\n","\n","$A^{[2]}=\\text{softmax}(Z^{[2]})$\n","</div>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YzBJgODl4aDp"},"source":["You have the flexibility to create any function within or outside the class, allowing you to modify parameters as needed"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":539,"status":"ok","timestamp":1705946133040,"user":{"displayName":"Jigyasa Chouhan","userId":"08477043685172285234"},"user_tz":-330},"id":"8A6ScQ-8lzWy"},"outputs":[],"source":["#importing libraries\n","import pandas as pd\n","import numpy as np\n","from keras.datasets import mnist\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing"]},{"cell_type":"markdown","metadata":{"id":"TGVN81yBnufX"},"source":["### Required functions"]},{"cell_type":"code","source":["x=np.array([[[1], [2], [3]], [[4], [5], [6]]])\n","print(x.shape)\n","sumarr=np.sum(x, axis=1)\n","print(sumarr.shape)\n","print(sumarr)\n","print(x/sumarr.reshape((2,1,1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dFKxnOclIRLv","executionInfo":{"status":"ok","timestamp":1705946133775,"user_tz":-330,"elapsed":5,"user":{"displayName":"Jigyasa Chouhan","userId":"08477043685172285234"}},"outputId":"2e66083a-5d21-4b76-9bc3-eecf8e13647a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 3, 1)\n","(2, 1)\n","[[ 6]\n"," [15]]\n","[[[0.16666667]\n","  [0.33333333]\n","  [0.5       ]]\n","\n"," [[0.26666667]\n","  [0.33333333]\n","  [0.4       ]]]\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705946133775,"user":{"displayName":"Jigyasa Chouhan","userId":"08477043685172285234"},"user_tz":-330},"id":"N4A3X8zjh5My"},"outputs":[],"source":["# activation and loss functions\n","n=10000\n","\n","def ReLU(x):\n","  return np.maximum(x, 0)\n","\n","def derivative_ReLU(x):\n","  return np.where(x>=0, 1, 0)\n","\n","def softmax(arr):\n","  return (np.exp(arr)).astype(np.float64)/(np.sum(np.exp(arr), axis=1).reshape(n,1,1)).astype(np.float64)\n","\n","def CrossEntropy(pred, actual):\n","  sum=0.0\n","  for i in range(10):\n","    sum=sum+actual[i]*np.log(pred[i])\n","  return -1*sum\n","\n","def AvgError(pred, actual):\n","  n_samples=len(pred)\n","  sum=0\n","  for i in range(n_samples):\n","    sum=sum-CrossEntropy(pred[i], actual[i])\n","  return -1*sum/float(n_samples)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705946133775,"user":{"displayName":"Jigyasa Chouhan","userId":"08477043685172285234"},"user_tz":-330},"id":"Vqdn0Wv0mFNE"},"outputs":[],"source":["#complete the class of neural network\n","\n","class NN:\n","  def __init__(self):\n","      self.w1=np.zeros((n, 10, 784), dtype=\"float64\")\n","      self.b1=np.zeros((n, 10, 1), dtype=\"float64\")\n","      self.w2=np.zeros((n, 10, 10), dtype=\"float64\")\n","      self.b2=np.zeros((n, 10, 1), dtype=\"float64\")\n","      self.a0=np.zeros((n, 784, 1), dtype=\"float64\")\n","      self.a1=np.zeros((n, 10,1), dtype=\"float64\")\n","      self.a2=np.zeros((n, 10,1), dtype=\"float64\")\n","      self.z1=np.zeros((n, 10,1), dtype=\"float64\")\n","      self.z2=np.zeros((n, 10,1), dtype=\"float64\")\n","      self.der_loss_w2=None\n","      self.der_loss_b2=None\n","      self.der_loss_w1=None\n","      self.der_loss_b1=None\n","\n","  def forward_propagation(self, input):\n","      self.a0=input.reshape(n, 784, 1).astype(np.float64)\n","      self.z1=(np.matmul(self.w1, self.a0) + self.b1).astype(np.float64)\n","      self.a1=(ReLU(self.z1)).astype(np.float64)\n","      self.z2=(np.matmul(self.w2, self.a1) + self.b2).astype(np.float64)\n","      self.a2=softmax(self.z2).astype(np.float64)\n","      # print(type(self.a0[0][0][0]))\n","\n","  def one_hot(self, y): #return a 0 vector with 1 only in the position corresponding to the value in test target\n","      return np.eye(10)[y].astype(np.float64)\n","\n","  # def backward_propagation(self, t):\n","  #     t=t.reshape(n, 10, 1).astype(np.float64)\n","  #     sum=np.sum(np.exp(self.z2), axis=1).astype(np.float64)\n","  #     sum=sum.reshape(n,1,1)\n","  #     self.der_loss_b2=(np.exp(self.z2)/sum - t)\n","  #     self.der_loss_w2=(np.matmul(np.exp(self.z2), np.transpose(self.a1, (0,2,1)))/sum - np.matmul(t, np.transpose(self.a1, (0,2,1))))\n","\n","  #     self.der_loss_b1=(derivative_ReLU(self.z1) * ((1/sum)*np.matmul(np.transpose(np.exp(self.z2), (0,2,1)), self.w2) - np.matmul(np.transpose(t, (0,2,1)), self.w2)).reshape(n, 10,1))\n","  #     self.der_loss_w1=(np.matmul(derivative_ReLU(self.z1), np.transpose(self.a0, (0,2,1))) * ((1/sum)*np.matmul(np.transpose(np.exp(self.z2), (0,2,1)), self.w2) - np.matmul(np.transpose(t, (0,2,1)), self.w2)).reshape(n, 10,1))\n","\n","  def backward_propagation(self, t):\n","    \"\"\"\n","    Performs backpropagation to calculate gradients of loss with respect to weights and biases.\n","\n","    Args:\n","      t: Target output (one-hot encoded).\n","    \"\"\"\n","\n","    t = t.reshape(-1, 10, 1)  # Reshape target for compatibility\n","\n","    # Output layer (softmax)\n","    sum_exp_z2 = np.sum(np.exp(self.z2), axis=1, keepdims=True)\n","    dz2 = (np.exp(self.z2) / sum_exp_z2) - t  # Derivative of softmax loss\n","    a1transpose=np.transpose(self.a1, (0,2,1))\n","    dw2 = [np.matmul(slice1, slice2) for slice1, slice2 in zip(dz2, a1transpose)]\n","    dw2=np.array(dw2)\n","    # dw2 = np.einsum(dz2, self.a1.T)  # Gradient of loss w.r.t. w2\n","    db2=(np.exp(self.z2)/sum_exp_z2 - t)\n","    # db2 = np.sum(dz2, axis=(0, 2), keepdims=True)  # Gradient of loss w.r.t. b2\n","\n","    # Hidden layer (ReLU)\n","    w2transpose=np.transpose(self.w2, (0,2,1))\n","    dz1 = [np.matmul(slice1, slice2) for slice1, slice2 in zip(w2transpose, dz2)]\n","    dz1 = np.array(dz1)\n","    # dz1 = np.matmul(self.w2.T, dz2) * derivative_ReLU(self.z1)  # Backpropagate through ReLU\n","    a0transpose = np.transpose(self.a0, (0,2,1))\n","    print(dz1.shape)\n","    print(a0transpose.shape)\n","    dw1 = [np.matmul(slice1, slice2) for slice1, slice2 in zip(dz1, a0transpose)]\n","    dw1 = np.array(dw1)\n","    # dw1 = np.matmul(dz1, self.a0.T)  # Gradient of loss w.r.t. w1\n","    db1=(derivative_ReLU(self.z1) * ((1/sum_exp_z2)*np.matmul(np.transpose(np.exp(self.z2), (0,2,1)), self.w2) - np.matmul(np.transpose(t, (0,2,1)), self.w2)).reshape(n, 10,1))\n","\n","    # db1 = np.sum(dz1, axis=(0, 2), keepdims=True)  # Gradient of loss w.r.t. b1\n","    self.der_loss_w2 = dw2\n","    self.der_loss_b2 = db2\n","    self.der_loss_w1 = dw1\n","    self.der_loss_b1 = db1\n","\n","\n","  def update_params(self, xtrain, ytrain):\n","      reqdy=self.one_hot(ytrain)\n","      self.forward_propagation(xtrain)\n","      self.backward_propagation(reqdy)\n","      self.gradient_descent()\n","\n","  def get_accuracy(self, y_pred, y_actual):\n","      n_samples=len(y_pred)\n","      sum=0.0\n","      for i in range(n_samples):\n","        if y_pred[i]==y_actual[i]:\n","          sum+=1\n","      return float(sum)/n_samples\n","\n","  def gradient_descent(self, lr=0.05, niter=1000):\n","    der_loss_w1avg=np.mean(self.der_loss_w1, axis=0).astype(np.float64)\n","    der_loss_b1avg=np.mean(self.der_loss_b1, axis=0).astype(np.float64)\n","    der_loss_w2avg=np.mean(self.der_loss_w2, axis=0).astype(np.float64)\n","    der_loss_b2avg=np.mean(self.der_loss_b2, axis=0).astype(np.float64)\n","    for _ in range(niter):\n","        self.w1=self.w1-lr*der_loss_w1avg\n","        self.b1=self.b1-lr*der_loss_b1avg\n","        self.w2=self.w2-lr*der_loss_w2avg\n","        self.b2=self.b2-lr*der_loss_b2avg\n","\n","  def make_predictions(self, xtest):\n","      self.forward_propagation(xtest)\n","      print(self.a2)\n","      y_pred=np.argmax(self.a2, axis=1).reshape(-1)\n","      return y_pred\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FsgNaz6qmoLI"},"source":["## main"]},{"cell_type":"code","source":["nn=NN()\n","print(nn.one_hot([1,2,3]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjKWMQLHP0Lr","executionInfo":{"status":"ok","timestamp":1705946133775,"user_tz":-330,"elapsed":3,"user":{"displayName":"Jigyasa Chouhan","userId":"08477043685172285234"}},"outputId":"96cf20d3-009d-4ff1-d73a-20f51633f79d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":770,"status":"ok","timestamp":1705946134543,"user":{"displayName":"Jigyasa Chouhan","userId":"08477043685172285234"},"user_tz":-330},"id":"iIbC5z1Lmlcr"},"outputs":[],"source":["(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","X_train=X_train.reshape(60000,784)/255.0\n","X_test=X_test.reshape(10000,784)/255.0\n"]},{"cell_type":"markdown","metadata":{"id":"ymme4NNNmws9"},"source":["###preprocessing the data\n"]},{"cell_type":"markdown","metadata":{"id":"B5dqfE25m7ZD"},"source":["###Model Training"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G05ggxM1m_n0","executionInfo":{"status":"ok","timestamp":1705947491176,"user_tz":-330,"elapsed":1356636,"user":{"displayName":"Jigyasa Chouhan","userId":"08477043685172285234"}},"outputId":"c38210a6-e094-4383-a74d-41123e50913d"},"outputs":[{"output_type":"stream","name":"stdout","text":["training iteration 0\n","(10000, 10, 1)\n","(10000, 1, 784)\n","training iteration 1\n","(10000, 10, 1)\n","(10000, 1, 784)\n","training iteration 2\n","(10000, 10, 1)\n","(10000, 1, 784)\n","training iteration 3\n","(10000, 10, 1)\n","(10000, 1, 784)\n","training iteration 4\n","(10000, 10, 1)\n","(10000, 1, 784)\n","training iteration 5\n","(10000, 10, 1)\n","(10000, 1, 784)\n"]}],"source":[" #training model using gradient descent\n","MyNetwork=NN()\n","\n","for i in range(6):\n","  print(\"training iteration\", i)\n","  MyNetwork.update_params(X_train[i*n:(i+1)*n], Y_train[i*n:(i+1)*n])\n"]},{"cell_type":"markdown","metadata":{"id":"sa-CT3UnnAsr"},"source":["### Viewing Results\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WV9UEIHbnJKd","executionInfo":{"status":"ok","timestamp":1705947541118,"user_tz":-330,"elapsed":461,"user":{"displayName":"Jigyasa Chouhan","userId":"08477043685172285234"}},"outputId":"efce7379-962f-4ad3-a6a8-990a224f264d"},"outputs":[{"output_type":"stream","name":"stdout","text":["test iteration 0\n","[[[4.73452336e-03]\n","  [1.37132472e-01]\n","  [4.59357628e-01]\n","  ...\n","  [1.00727118e-01]\n","  [4.65260808e-06]\n","  [7.96252256e-02]]\n","\n"," [[4.73452336e-03]\n","  [1.37132472e-01]\n","  [4.59357628e-01]\n","  ...\n","  [1.00727118e-01]\n","  [4.65260808e-06]\n","  [7.96252256e-02]]\n","\n"," [[4.73452336e-03]\n","  [1.37132472e-01]\n","  [4.59357628e-01]\n","  ...\n","  [1.00727118e-01]\n","  [4.65260808e-06]\n","  [7.96252256e-02]]\n","\n"," ...\n","\n"," [[4.73452336e-03]\n","  [1.37132472e-01]\n","  [4.59357628e-01]\n","  ...\n","  [1.00727118e-01]\n","  [4.65260808e-06]\n","  [7.96252256e-02]]\n","\n"," [[4.73452336e-03]\n","  [1.37132472e-01]\n","  [4.59357628e-01]\n","  ...\n","  [1.00727118e-01]\n","  [4.65260808e-06]\n","  [7.96252256e-02]]\n","\n"," [[4.73452336e-03]\n","  [1.37132472e-01]\n","  [4.59357628e-01]\n","  ...\n","  [1.00727118e-01]\n","  [4.65260808e-06]\n","  [7.96252256e-02]]]\n","[2. 2. 2. ... 2. 2. 2.]\n","0.1032\n"]}],"source":["#viewing prediction for 10 random images in dataset\n","predictions=np.array([])\n","for i in range(1):\n","  print(\"test iteration\", i)\n","  predictions=np.append(predictions, MyNetwork.make_predictions(X_test[i*n:(i+1)*n]))\n","\n","predictions.reshape(-1)\n","acc=MyNetwork.get_accuracy(predictions, Y_test)\n","print(predictions)\n","print(acc)\n"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}